
# üîÑ DRC Data Replication Pipeline  
### Using Apache Spark, HDFS & Airflow in a Simulated Disaster Recovery Cluster

A containerized data engineering solution that replicates transactional data from a **Data Center (DC)** to a **Disaster Recovery Center (DRC)**. This project uses **Apache Spark** for data processing, **HDFS** for storage, and **Apache Airflow** for orchestrating the ETL pipeline. 

This project is designed for learning, simulation, or as a base for enterprise-grade disaster recovery pipelines.

---

## üìå Overview

This project simulates a real-world disaster recovery setup where data generated in a primary cluster is automatically synced to a secondary backup cluster (DRC). It includes:

- Spark jobs to generate and replicate data  
- HDFS clusters for both Primary and Secondary DCs  
- Airflow DAGs for orchestration and monitoring  
- Docker-based deployment for ease of use

---

## üèóÔ∏è Architecture

```text
+-----------------------------+
|     Apache Airflow DAG     |
| +-----------------------+  |
| | Generate PARQUET Data |  |  
| +-----------------------+  |
| | Submit Spark Job      |  |
| +-----------------------+  |
| | Validate Output       |  |
+-------------|---------------+
              |
              v
+-----------------------------+   sync to   +-----------------------------+
| Primary DC (HDFS + Spark)  |  ----------> | Secondary DC (HDFS / DRC)   |
+-----------------------------+             +-----------------------------+
```

---

## ‚öôÔ∏è Services

| Service                         | Role                    | Port(s)       | Profile       |
|---------------------------------|--------------------------|---------------|---------------|
| HDFS NameNode (Primary DC)      | Storage Manager          | 11870, 11020  | primary-dc    |
| HDFS DataNode (Primary DC)      | Data Storage Node        | 11864         | primary-dc    |
| Spark Master                    | Job Driver               | 11190, 11077  | primary-dc    |
| Spark Worker                    | Job Executor             | 11191         | primary-dc    |
| Airflow Webserver               | ETL Orchestrator UI      | 11080         | primary-dc    |
| Airflow Scheduler               | DAG Trigger              | (internal)    | primary-dc    |
| PostgreSQL                      | Airflow Backend          | (internal)    | primary-dc    |
| HDFS NameNode (Secondary DC)    | Backup Storage Manager   | 21870, 21020  | secondary-dc  |
| HDFS DataNode (Secondary DC)    | Backup Data Storage      | 21864         | secondary-dc  |

---

## üìÅ Project Structure

```
drc-project/
‚îú‚îÄ‚îÄ airflow/                            # DAGs, logs, plugins
‚îÇ   ‚îî‚îÄ‚îÄ dags/
‚îÇ       ‚îú‚îÄ‚îÄ bank-transaction-hdfs-ingest-dag.py
‚îÇ       ‚îî‚îÄ‚îÄ dcr-sync-dag.py
‚îú‚îÄ‚îÄ spark-jobs/                         # PySpark job scripts
‚îÇ   ‚îú‚îÄ‚îÄ bank-transaction-hdfs-ingest-script.py
|   ‚îú‚îÄ‚îÄ dcr-sync-script.py
‚îÇ   ‚îî‚îÄ‚îÄ dcr-project.env
‚îú‚îÄ‚îÄ spark/                              # Spark app directory (bind mounted)
‚îú‚îÄ‚îÄ hadoop-primary/                     # HDFS for primary DC
‚îú‚îÄ‚îÄ hadoop-secondary/                   # HDFS for secondary DRC
‚îú‚îÄ‚îÄ postgres/                           # PostgreSQL volume
‚îú‚îÄ‚îÄ Dockerfile.airflow                  # Custom Airflow image
‚îú‚îÄ‚îÄ Dockerfile.spark                    # Custom Spark image
‚îú‚îÄ‚îÄ Dockerfile.hadoop-namenode
‚îú‚îÄ‚îÄ Dockerfile.hadoop-datanode
‚îú‚îÄ‚îÄ docker-compose.yml                  # Service definitions
‚îú‚îÄ‚îÄ requirements.txt                    # Spark job dependencies
‚îú‚îÄ‚îÄ requirements-airflow.txt            # Airflow-specific dependencies
‚îî‚îÄ‚îÄ README.md
```

---

## üîß Prerequisites

- Docker & Docker Compose
- 8 GB RAM or more recommended
- Ports 11080, 11870, 21870 etc. available

---


## üöÄ Getting Started

### 1. Clone the repository
```bash
git clone https://github.com/fajarafriad1/drc-data-replication-pipeline-using-spark-and-airflow.git
cd drc-data-replication-pipeline-using-spark-and-airflow
```

### 2. Build and start services

Build and start the Primary DC services:

```bash
docker compose --profile primary-dc up --build
```
Wait until the build and startup complete. Then build and start the Secondary DC services:

```bash
docker compose --profile secondary-dc up --build
```
### 3. Create HDFS Shared Directory

After both HDFS clusters are running, open a terminal in the NameNode containers and create the shared directory.

#### üîπ Primary DC

```bash
docker exec -it drc-project-primary-dc-namenode hdfs dfs -mkdir -p /user/shared-directory/ && \
docker exec -it drc-project-primary-dc-namenode hdfs dfs -chmod -R 777 /user/shared-directory/
```

#### üîπ Secondary DC

```bash
docker exec -it drc-project-secondary-dc-namenode hdfs dfs -mkdir -p /user/shared-directory/ && \
docker exec -it drc-project-secondary-dc-namenode hdfs dfs -chmod -R 777 /user/shared-directory/
```

### 4. Access Airflow

Open Airflow in your browser:

üìç http://localhost:11080

- **Username**: `admin`
- **Password**: `admin`


### 5. Fix Folder Permissions (Optional but Recommended)

If you encounter permission errors like:

```text
PermissionError: [Errno 13] Permission denied: ...'
```

Run the following:

```bash
sudo chmod -R 777 ./hadoop-primary/namenode ./hadoop-primary/datanode
sudo chmod -R 777 ./hadoop-secondary/namenode ./hadoop-secondary/datanode
sudo chmod -R 777 ./airflow/dags
sudo chmod -R 777 ./spark-jobs
```

This ensures Airflow and Spark containers can access necessary volumes.

---

## üìã Airflow DAGs (Example Usecase)

### üü¢ `bank-transaction-hdfs-ingest-dag`
- Generates dummy bank transactions and convert to dataframe using PySpark  
- Writes output as PARQUET to Primary HDFS

### üîÅ `dcr-sync-dag`
- Waits for the first job to finish (using ExternalTaskSensor in Airflow)
- Reads PARQUET from Primary DC HDFS  
- Writes replicated data to DRC (Secondary DC HDFS)

---

## üîå Airflow Spark Connection Configuration

To enable **Apache Airflow** to submit Spark jobs, create a Spark connection via the **Airflow UI** or using the **Airflow CLI/API**.

> Navigate to **Admin ‚Üí Connections** and add the following configuration:

| **Field**         | **Value**                                                                 |
|-------------------|---------------------------------------------------------------------------|
| **Connection Id** | `spark_client_conn`                                                       |
| **Connection Type** | `Spark` *(Install provider: `apache-airflow-providers-apache-spark`)* |
| **Host**          | `spark://drc-project-primary-dc-spark-master`                             |
| **Port**          | `7077`                                                                    |
| **Deploy Mode**   | `client` *(Options: `client` or `cluster`)*                               |
| **Spark Binary**  | `spark-submit` *(Options: `spark-submit`, `spark2-submit`, `spark3-submit`)* |
| **YARN Queue**    | *(optional, leave blank if not using YARN)*                               |
| **Description**   | Spark connection for submitting jobs from Airflow                         |


This connection is used by the `SparkSubmitOperator` in your Airflow DAGs to launch Spark jobs against the Spark master running in the **Primary DC** container.

---

## üß™ Sample .env (`drc-project.env`)

```env
# Primary Data Center HDFS URI
PRIMARY_DC=hdfs://drc-project-primary-dc-namenode:8020/

# Secondary Data Center HDFS URI
SECONDARY_DC=hdfs://drc-project-secondary-dc-namenode:8020/

# Directory path in HDFS
HDFS_PATH=/user/shared-directory/

# Name of the transaction data file
FILENAME=bank_transactions.parquet
```

---

## üß∞ Built With

- [Apache Spark](https://spark.apache.org/)
- [Apache Airflow](https://airflow.apache.org/)
- [Hadoop HDFS](https://hadoop.apache.org/)
- [Docker Compose](https://docs.docker.com/compose/)

---

## üìÑ License

This project is licensed under the [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0).

---

## üôã‚Äç‚ôÇÔ∏è Author

**Fajar Afriadi**  
_Data Engineer_
üì´ [LinkedIn](https://www.linkedin.com/in/fajar-afriadi/)
